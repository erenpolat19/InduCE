{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import dense_to_sparse, degree\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.classificationnet import GCNSynthetic\n",
    "from utils.utils import normalize_adj, get_neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "header = [\"node_idx\", \"new_idx\", \"cf_adj\", \"sub_adj\", \"cf\", \"num_nodes\", \"node_dict\", \"y_pred\", \"y_pred_cf\",\n",
    "            \"label\",  \"loss_total\", \"loss_pred\", \"loss_graph_dist\", \"found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set y_true counts: (array([0, 1]), array([96, 79]))\n",
      "test set y_pred_orig counts: (array([0, 1]), array([94, 81]))\n",
      "Whole graph counts: (array([0, 1]), array([511, 360]))\n"
     ]
    }
   ],
   "source": [
    "# For original model\n",
    "dataset = \"syn4\"\n",
    "hidden = 20\n",
    "seed = 37\n",
    "dropout = 0.0\n",
    "\n",
    "# Load original dataset and model\n",
    "\n",
    "with open(\"../../data/gnn_explainer/{}.pickle\".format(dataset), \"rb\") as f:\n",
    "\tdata = pickle.load(f)\n",
    "\n",
    "adj = torch.Tensor(data[\"adj\"]).squeeze()       # Does not include self loops\n",
    "features = torch.Tensor(data[\"feat\"]).squeeze()\n",
    "labels = torch.tensor(data[\"labels\"]).squeeze()\n",
    "idx_train = torch.tensor(data[\"train_idx\"])\n",
    "idx_test = torch.tensor(data[\"test_idx\"])\n",
    "edge_index = dense_to_sparse(adj)\n",
    "\n",
    "norm_adj = normalize_adj(adj)\n",
    "\n",
    "model = GCNSynthetic(nfeat=features.shape[1], nhid=hidden, nout=hidden,\n",
    "                     nclass=len(labels.unique()), dropout=dropout)\n",
    "model.load_state_dict(torch.load(\"../../models/gcn_3layer_{}.pt\".format(dataset)))\n",
    "model.eval()\n",
    "output = model(features, norm_adj)\n",
    "y_pred_orig = torch.argmax(output, dim=1)\n",
    "print(\"test set y_true counts: {}\".format(np.unique(labels[idx_test].numpy(), return_counts=True)))\n",
    "print(\"test set y_pred_orig counts: {}\".format(np.unique(y_pred_orig[idx_test].numpy(), return_counts=True)))\n",
    "print(\"Whole graph counts: {}\".format(np.unique(labels.numpy(), return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING RESULTS TO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prefix = 'inductive_non0_correct_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=False #keep true if training and then testing, else keep False when directly testing using saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    # Load cf examples for train set\n",
    "    with open(\"../../results/{}/{}_train_{}.pkl\".format(dataset, dataset, save_prefix), \"rb\") as ftrain:\n",
    "        cf_examples_train = pickle.load(ftrain)\n",
    "    ftrain.close()\n",
    "\n",
    "# Load cf examples for test set\n",
    "with open(\"../../results/{}/{}_test_{}.pkl\".format(dataset, dataset, save_prefix), \"rb\") as ftest:\n",
    "    cf_examples_test = pickle.load(ftest)\n",
    "ftest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    df_train = pd.DataFrame(columns=header)\n",
    "    df_train_not_found = pd.DataFrame(columns=header)\n",
    "\n",
    "    for key in cf_examples_train.keys():\n",
    "        val={}\n",
    "        i=0\n",
    "        for head in header:\n",
    "            if(head == \"node_idx\"):\n",
    "                val[head] = key\n",
    "            else:\n",
    "                if(torch.is_tensor(cf_examples_train[key][i])):\n",
    "                    val[head] = cf_examples_train[key][i].detach().cpu().numpy()\n",
    "                else:\n",
    "                    val[head] = cf_examples_train[key][i]\n",
    "                i+=1\n",
    "        if(val['found'] == 'not found'):\n",
    "            df_train_not_found = df_train_not_found.append(val, ignore_index=True)\n",
    "        else:\n",
    "            df_train = df_train.append(val, ignore_index=True)\n",
    "\n",
    "    print(len(df_train), len(df_train_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(columns=header)\n",
    "df_test_not_found = pd.DataFrame(columns=header)\n",
    "for key in cf_examples_test.keys():\n",
    "    val={}\n",
    "    i=0\n",
    "    for head in header:\n",
    "        if(head == \"node_idx\"):\n",
    "            val[head] = key\n",
    "        else:\n",
    "            if(torch.is_tensor(cf_examples_test[key][i])):\n",
    "                val[head] = cf_examples_test[key][i].detach().cpu().numpy()\n",
    "            else:\n",
    "                val[head] = cf_examples_test[key][i]\n",
    "            i+=1\n",
    "    if(val['found'] == 'not found'):\n",
    "        df_test_not_found = df_test_not_found.append(val, ignore_index=True)\n",
    "    else:\n",
    "        df_test = df_test.append(val, ignore_index=True)\n",
    "\n",
    "len(df_test), len(df_test_not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUANTITATIVE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cf examples found during test: 72/72\n",
      "Average graph distance test: 2.3055555555555554\n"
     ]
    }
   ],
   "source": [
    "if(train):\n",
    "    print(\"Num cf examples found during train: {}/{}\".format(len(df_train), len(df_train)+len(df_train_not_found)))\n",
    "print(\"Num cf examples found during test: {}/{}\".format(len(df_test), len(df_test)+len(df_test_not_found)))\n",
    "if(train):\n",
    "    print(\"Average graph distance train: {}\".format(np.mean(df_train[\"loss_graph_dist\"])))\n",
    "print(\"Average graph distance test: {}\".format(np.mean(df_test[\"loss_graph_dist\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG EXPLANATION SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "  avg_cf_len = 0\n",
    "  cf_size_list_train = []\n",
    "  cf_size_dict_train = {}\n",
    "  for i in range(len(df_train)) :\n",
    "    cf_len = len(df_train.loc[i, \"cf\"])\n",
    "    avg_cf_len += cf_len\n",
    "    cf_size_list_train.append(cf_len)\n",
    "    label = df_train.loc[i, \"label\"].item()\n",
    "    if(label not in cf_size_dict_train.keys()):\n",
    "      cf_size_dict_train[label] = [cf_len]\n",
    "    else:\n",
    "      cf_size_dict_train[label].append(cf_len)\n",
    "\n",
    "  #check both mean vals to be equal\n",
    "  print(\"Average counterfactual length train: \", np.mean(cf_size_list_train), avg_cf_len/len(df_train))\n",
    "  print(\"Std Dev counterfactual length train: \", np.std(cf_size_list_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    for key in cf_size_dict_train.keys():\n",
    "        print(\"Label: {}, Avg CF Size: {:.2f} +- {:2f}\".format(key, np.mean(cf_size_dict_train[key]), np.std(cf_size_dict_train[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average counterfactual length test:  2.3055555555555554 2.3055555555555554\n",
      "Std Dev counterfactual length test:  1.4398966869660483\n"
     ]
    }
   ],
   "source": [
    "avg_cf_len = 0\n",
    "cf_size_list_test = []\n",
    "cf_size_dict_test = {}\n",
    "for i in range(len(df_test)) :\n",
    "  cf_len = len(df_test.loc[i, \"cf\"])\n",
    "  avg_cf_len += cf_len\n",
    "  cf_size_list_test.append(cf_len)\n",
    "  label = df_test.loc[i, \"label\"].item()\n",
    "  if(label not in cf_size_dict_test.keys()):\n",
    "    cf_size_dict_test[label] = [cf_len]\n",
    "  else:\n",
    "    cf_size_dict_test[label].append(cf_len)\n",
    "\n",
    "\n",
    "print(\"Average counterfactual length test: \", np.mean(cf_size_list_test), avg_cf_len/len(df_test))\n",
    "print(\"Std Dev counterfactual length test: \", np.std(cf_size_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1, Avg CF Size: 2.31 +- 1.44\n"
     ]
    }
   ],
   "source": [
    "for key in cf_size_dict_test.keys():\n",
    "    print(\"Label: {}, Avg CF Size: {:.2f} +- {:.2f}\".format(key, np.mean(cf_size_dict_test[key]), np.std(cf_size_dict_test[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum cf len test:  7\n"
     ]
    }
   ],
   "source": [
    "if(train):\n",
    "    print('Maximum cf len train: ', max(cf_size_list_train))\n",
    "print('Maximum cf len test: ', max(cf_size_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIDELITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AVERAGE FIDELITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg fidelity test: 0.000\n"
     ]
    }
   ],
   "source": [
    "if(train):\n",
    "    print(\"Avg fidelity train: {:.3f}\".format(1 - len(df_train) / (len(df_train)+len(df_train_not_found))))\n",
    "print(\"Avg fidelity test: {:.3f}\".format(1 - len(df_test) / (len(df_test)+len(df_test_not_found))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL-WISE FIDELITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    label_found_train = [len(df_train[df_train['label'] == 0]),len(df_train[df_train['label'] == 1])] \n",
    "    print(label_found_train)\n",
    "    label_not_found_train = [len(df_train_not_found[df_train_not_found['label'] == 0]),\n",
    "    len(df_train_not_found[df_train_not_found['label'] == 1])]\n",
    "    print(label_not_found_train)\n",
    "    ## label wise fidelity\n",
    "    for i in range(len(label_found_train)):\n",
    "        if(label_found_train[i]+label_not_found_train[i] == 0):\n",
    "            continue\n",
    "        fidelity = 1 - (label_found_train[i]/(label_found_train[i]+label_not_found_train[i]))\n",
    "        print('Fidelity of label {} (train) : {:.3f}'.format(i, fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 72]\n",
      "[0, 0]\n",
      "Fidelity of label 1 (test) : 0.000\n"
     ]
    }
   ],
   "source": [
    "label_found_test = [len(df_test[df_test['label'] == 0]),len(df_test[df_test['label'] == 1])] \n",
    "print(label_found_test)\n",
    "label_not_found_test = [len(df_test_not_found[df_test_not_found['label'] == 0]),\n",
    "len(df_test_not_found[df_test_not_found['label'] == 1])]\n",
    "print(label_not_found_test)\n",
    "## label wise fidelity\n",
    "for i in range(len(label_found_test)):\n",
    "    if(label_found_test[i]+label_not_found_test[i] == 0):\n",
    "        continue\n",
    "    fidelity = 1 - (label_found_test[i]/(label_found_test[i]+label_not_found_test[i]))\n",
    "    print('Fidelity of label {} (test) : {:.3f}'.format(i, fidelity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(train):\n",
    "    accuracy = []\n",
    "    # Get original predictions \n",
    "    dict_ypred_orig = dict(zip(sorted(np.concatenate((idx_train.numpy(), idx_test.numpy()))), \n",
    "                                y_pred_orig.numpy()))\n",
    "\n",
    "    for i in range(len(df_train)): \n",
    "        node_idx = df_train[\"node_idx\"][i]\n",
    "        new_idx = df_train[\"new_idx\"][i]\n",
    "\n",
    "        _, _, _, node_dict = get_neighbourhood(int(node_idx), edge_index, 4, features, labels)\n",
    "\n",
    "        # Confirm idx mapping is correct\n",
    "        if node_dict[node_idx] == df_train[\"new_idx\"][i]:\n",
    "            cf = df_train[\"cf\"][i]\n",
    "            correct=0\n",
    "            nodes_in_motif = []\n",
    "            for perb in cf:\n",
    "                u = perb[0]\n",
    "                v = perb[1]\n",
    "                if(perb[2] == 'add'): #assuming all additions to be correct\n",
    "                    correct+=1\n",
    "                else: #deletion only between motif node as correct, u is the target node, v is the other node\n",
    "                    if(dict_ypred_orig[v] != 0 and dict_ypred_orig[u] != 0):\n",
    "                        correct+=1\n",
    "                        nodes_in_motif.append(v)\n",
    "            correct = correct/len(cf)\n",
    "            \n",
    "            accuracy.append([node_idx, new_idx, cf, nodes_in_motif, correct])\n",
    "            # print(correct)\n",
    "            # print(accuracy)\n",
    "    df_accuracy = pd.DataFrame(accuracy, columns=[\"node_idx\", \"new_idx\", \"cf\", \"nodes_in_motif\", \"prop_correct\"])\n",
    "    print(\"model: {}, Accuracy: {:.4f}\".format(save_prefix, np.mean(df_accuracy[\"prop_correct\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: inductive_non0_correct_only, Accuracy: 0.9665\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "# Get original predictions \n",
    "dict_ypred_orig = dict(zip(sorted(np.concatenate((idx_train.numpy(), idx_test.numpy()))), \n",
    "                            y_pred_orig.numpy()))\n",
    "for i in range(len(df_test)): \n",
    "    node_idx = df_test[\"node_idx\"][i]\n",
    "    new_idx = df_test[\"new_idx\"][i]\n",
    "\n",
    "    _, _, _, node_dict = get_neighbourhood(int(node_idx), edge_index, 4, features, labels)\n",
    "\n",
    "    # Confirm idx mapping is correct\n",
    "    if node_dict[node_idx] == df_test[\"new_idx\"][i]:\n",
    "        cf = df_test[\"cf\"][i]\n",
    "        correct=0\n",
    "        nodes_in_motif = []\n",
    "        for perb in cf:\n",
    "            u = perb[0]\n",
    "            v = perb[1]\n",
    "            if(perb[2] == 'add'): #assuming all additions to be correct\n",
    "                correct+=1\n",
    "            else: #deletion only between motif node as correct, u is the target node, v is the other node\n",
    "                if(dict_ypred_orig[v] != 0 and dict_ypred_orig[u] != 0):\n",
    "                    correct+=1\n",
    "                    nodes_in_motif.append(v)\n",
    "        correct = correct/len(cf)\n",
    "        \n",
    "        accuracy.append([node_idx, new_idx, cf, nodes_in_motif, correct])\n",
    "        # print(correct)\n",
    "        # print(accuracy)\n",
    "df_accuracy_test = pd.DataFrame(accuracy, columns=[\"node_idx\", \"new_idx\", \"cf\", \"nodes_in_motif\", \"prop_correct\"])\n",
    "print(\"model: {}, Accuracy: {:.4f}\".format(save_prefix, np.mean(df_accuracy_test[\"prop_correct\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('GNNEx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "105d8bb4a76a1442d8cf35c928fae7818a4dd3cfc700eff9a221d54f70f6f9f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
